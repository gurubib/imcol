{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.engine import Layer\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import RepeatVector, Permute\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 7434\n",
      "Length of validation set: 929\n",
      "Length of test set: 930\n"
     ]
    }
   ],
   "source": [
    "all_id = list(range(1000, 10293))\n",
    "\n",
    "valid_split = 0.1\n",
    "test_split = 0.1\n",
    "\n",
    "v_index = int(len(all_id)*(1-valid_split-test_split))\n",
    "t_index = int(len(all_id)*(1-test_split))\n",
    "\n",
    "train_ids = all_id[:v_index]\n",
    "valid_ids = all_id[v_index:t_index]\n",
    "test_ids = all_id[t_index:]\n",
    "\n",
    "print(\"Length of train set: \" + str(len(train_ids)))\n",
    "print(\"Length of validation set: \" + str(len(valid_ids)))\n",
    "print(\"Length of test set: \" + str(len(test_ids)))\n",
    "\n",
    "partition = {'train': train_ids, 'validation': valid_ids, 'test': test_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load weights\n",
    "inception = InceptionResNetV2(weights='imagenet', include_top=True)\n",
    "inception.graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_input = Input(shape=(1000,), name='embed_input')\n",
    "\n",
    "#Encoder\n",
    "encoder_input = Input(shape=(256, 256, 1,), name='encoder_input')\n",
    "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "\n",
    "#Fusion\n",
    "fusion_output = RepeatVector(32 * 32)(embed_input) \n",
    "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
    "fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
    "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output) \n",
    "\n",
    "#Decoder\n",
    "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 32s 16s/step - loss: 0.4793 - val_loss: 0.9879\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 14s 7s/step - loss: 1.0116 - val_loss: 0.9877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ee8ed1e320>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_inception_embedding(grayscaled_rgb):\n",
    "    grayscaled_rgb_resized = []\n",
    "    for i in grayscaled_rgb:\n",
    "        i = resize(i, (299, 299, 3), mode='constant')\n",
    "        #print(i.shape)\n",
    "        grayscaled_rgb_resized.append(i)\n",
    "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
    "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
    "    with inception.graph.as_default():\n",
    "        embed = inception.predict(grayscaled_rgb_resized)\n",
    "    return embed\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=32, dim=(256,256,1), shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *(256,256,1)), dtype=float)\n",
    "        Y = np.empty((self.batch_size, *(256,256,2)), dtype=float)\n",
    "        I = np.empty((self.batch_size, 1000))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            img = img_to_array(load_img('floydhub_dataset/' + str(ID) + '.jpg'))\n",
    "            grayscaled_rgb = gray2rgb(rgb2gray(img))\n",
    "            grayscaled_rgb = grayscaled_rgb.reshape((1,*(grayscaled_rgb.shape)))\n",
    "            img = 1.0/255*img\n",
    "            img = rgb2lab(img)\n",
    "            gray = img[:,:,0]\n",
    "            ab = img[:,:,1:] / 128\n",
    "            \n",
    "            # Inception\n",
    "            I[i,] = create_inception_embedding(grayscaled_rgb)\n",
    "            \n",
    "            # Store sample\n",
    "            X[i,] = gray.reshape((256,256,1))\n",
    "\n",
    "            # Store class\n",
    "            Y[i,] = ab\n",
    "\n",
    "        return {'encoder_input': X, 'embed_input': I}, Y\n",
    "\n",
    "##A kepeket 10-esevel betoltogeto cucc (bazch_size, hogy hanyasaval toltse be)\n",
    "training_generator = DataGenerator(partition['train'], batch_size=100)\n",
    "valid_generator = DataGenerator(partition['validation'], batch_size=100)\n",
    "\n",
    "\n",
    "#Train model      \n",
    "model.compile(optimizer='rmsprop', loss='mse')\n",
    "model.fit_generator(training_generator, validation_data=valid_generator, epochs=100, steps_per_epoch=74, validation_steps=9) #a steps_per_epoch=ossz_kep/batch_size PL:100/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 53630 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\util\\dtype.py:141: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n",
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 42329 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 27112 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 29978 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    }
   ],
   "source": [
    "color_me = []\n",
    "for filename in os.listdir('Test/'):\n",
    "    color_me.append(img_to_array(load_img('Test/'+filename)))\n",
    "color_me = np.array(color_me, dtype=float)\n",
    "gray_me = gray2rgb(rgb2gray(1.0/255*color_me))\n",
    "color_me_embed = create_inception_embedding(gray_me)\n",
    "color_me = rgb2lab(1.0/255*color_me)[:,:,:,0]\n",
    "color_me = color_me.reshape(color_me.shape+(1,))\n",
    "\n",
    "\n",
    "# Test model\n",
    "output = model.predict([color_me, color_me_embed])\n",
    "#print(output)\n",
    "output = output * 128\n",
    "\n",
    "# Output colorizations\n",
    "for i in range(len(output)):\n",
    "    cur = np.zeros((256, 256, 3))\n",
    "    cur[:,:,0] = color_me[i][:,:,0]\n",
    "    cur[:,:,1:] = output[i]\n",
    "    #print(output[i])\n",
    "    imsave(\"result/img_\"+str(i)+\".png\", lab2rgb(cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
