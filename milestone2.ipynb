{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.engine import Layer\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import RepeatVector, Permute\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images\n",
    "#X = []\n",
    "#i = 0\n",
    "#for filename in os.listdir('Train/'):\n",
    "#    X.append(img_to_array(load_img('Train/'+filename)))\n",
    "#    i += 1\n",
    "#    if i > 10:\n",
    "#        break\n",
    "#X = np.array(X, dtype=float)\n",
    "#Xtrain = 1.0/255*X\n",
    "\n",
    "##Ossz hany kepbol tanuljon\n",
    "all_id = list(range(0, 10))\n",
    "\n",
    "valid_split = 0.1\n",
    "test_split = 0.1\n",
    "\n",
    "v_index = int(len(all_id)*(1-valid_split-test_split))\n",
    "t_index = int(len(all_id)*(1-test_split))\n",
    "\n",
    "train_ids = all_id[:v_index]\n",
    "valid_ids = all_id[v_index:t_index]\n",
    "test_ids = all_id[t_index:]\n",
    "\n",
    "partition = {'train': train_ids, 'validation': valid_ids, 'test': test_ids}\n",
    "\n",
    "#Load weights\n",
    "inception = InceptionResNetV2(weights='imagenet', include_top=True)\n",
    "inception.graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_input = Input(shape=(1000,), name='embed_input')\n",
    "\n",
    "#Encoder\n",
    "encoder_input = Input(shape=(256, 256, 1,), name='encoder_input')\n",
    "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "\n",
    "#Fusion\n",
    "fusion_output = RepeatVector(32 * 32)(embed_input) \n",
    "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
    "fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
    "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output) \n",
    "\n",
    "#Decoder\n",
    "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 28s 3s/step - loss: 0.7825\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.9404\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.9404\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.9404\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 20s 2s/step - loss: 0.9404\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.9404\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.9404\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.9404\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.9404\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.9404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e30d3e7710>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_inception_embedding(grayscaled_rgb):\n",
    "    grayscaled_rgb_resized = []\n",
    "    for i in grayscaled_rgb:\n",
    "        i = resize(i, (299, 299, 3), mode='constant')\n",
    "        #print(i.shape)\n",
    "        grayscaled_rgb_resized.append(i)\n",
    "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
    "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
    "    with inception.graph.as_default():\n",
    "        embed = inception.predict(grayscaled_rgb_resized)\n",
    "    return embed\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=32, dim=(256,256,1), shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *(256,256,1)), dtype=float)\n",
    "        Y = np.empty((self.batch_size, *(256,256,2)), dtype=float)\n",
    "        I = np.empty((self.batch_size, 1000))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            img = img_to_array(load_img('Train/' + str(ID) + '.jpg'))\n",
    "            grayscaled_rgb = gray2rgb(rgb2gray(img))\n",
    "            grayscaled_rgb = grayscaled_rgb.reshape((1,*(grayscaled_rgb.shape)))\n",
    "            img = 1.0/255*img\n",
    "            img = rgb2lab(img)\n",
    "            gray = img[:,:,0]\n",
    "            ab = img[:,:,1:] / 128\n",
    "            \n",
    "            # Inception\n",
    "            I[i,] = create_inception_embedding(grayscaled_rgb)\n",
    "            \n",
    "            # Store sample\n",
    "            X[i,] = gray.reshape((256,256,1))\n",
    "\n",
    "            # Store class\n",
    "            Y[i,] = ab\n",
    "\n",
    "        return {'encoder_input': X, 'embed_input': I}, Y\n",
    "\n",
    "# Image transformer\n",
    "#datagen = ImageDataGenerator(\n",
    "#        shear_range=0.2,\n",
    "#        zoom_range=0.2,\n",
    "#        rotation_range=20,\n",
    "#        horizontal_flip=True)\n",
    "\n",
    "#Generate training data\n",
    "#Ez az eredeti batch_size, nincs koze a mi uj batch_sizeunkhoz\n",
    "#batch_size = 10\n",
    "\n",
    "#def image_a_b_gen(batch_size):\n",
    "#    for batch in datagen.flow(Xtrain, batch_size=batch_size):\n",
    "#        grayscaled_rgb = gray2rgb(rgb2gray(batch))\n",
    "#        embed = create_inception_embedding(grayscaled_rgb)\n",
    "#        lab_batch = rgb2lab(batch)\n",
    "#        X_batch = lab_batch[:,:,:,0]\n",
    "#        X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
    "#        Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "#        #print(batch.shape)\n",
    "#        print(embed.shape)\n",
    "#        yield ([X_batch, embed], Y_batch)\n",
    "\n",
    "##A kepeket 10-esevel betoltogeto cucc (bazch_size, hogy hanyasaval toltse be)\n",
    "training_generator = DataGenerator(partition['train'], batch_size=1)\n",
    "\n",
    "#Train model      \n",
    "model.compile(optimizer='rmsprop', loss='mse')\n",
    "model.fit_generator(training_generator, epochs=10, steps_per_epoch=8) #a steps_per_epoch=ossz_kep/batch_size 100/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 53630 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\util\\dtype.py:141: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n",
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 44972 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 46742 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 43083 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 39997 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 32363 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 51333 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 56768 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 34819 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\barni\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 47075 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    }
   ],
   "source": [
    "color_me = []\n",
    "for filename in os.listdir('Test/'):\n",
    "    color_me.append(img_to_array(load_img('Test/'+filename)))\n",
    "color_me = np.array(color_me, dtype=float)\n",
    "gray_me = gray2rgb(rgb2gray(1.0/255*color_me))\n",
    "color_me_embed = create_inception_embedding(gray_me)\n",
    "color_me = rgb2lab(1.0/255*color_me)[:,:,:,0]\n",
    "color_me = color_me.reshape(color_me.shape+(1,))\n",
    "\n",
    "\n",
    "# Test model\n",
    "output = model.predict([color_me, color_me_embed])\n",
    "output = output * 128\n",
    "\n",
    "# Output colorizations\n",
    "for i in range(len(output)):\n",
    "    cur = np.zeros((256, 256, 3))\n",
    "    cur[:,:,0] = color_me[i][:,:,0]\n",
    "    cur[:,:,1:] = output[i]\n",
    "    imsave(\"result/img_\"+str(i)+\".png\", lab2rgb(cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
