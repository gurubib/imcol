{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLearning nagyHF II. milestone\n",
    "## Team - balit_learning : Gurubi Barnabás - DXEXVR, Mátyás Gergely - IL21NI, Horváth Ákos - DKILK6\n",
    "\n",
    "## Before reading this project report\n",
    "These code snippets are working and tested, although you cannot try them, by just running in this _ipynb_, considering this project has many dependencies, including a large dataset and predetermined directories.\n",
    "\n",
    "Albeit if you do the following instructions you can run it:\n",
    "* Download this _ipynb_ and put it in a directory (referenced as root from now and on)\n",
    "* Download the following dataset: https://drive.google.com/open?id=1MYsSbdCOKQ8yJYQWhTPE9DGe_rcCaT4P then extract them into a directory named \"floydhub_dataset\"\n",
    "* Create a directory named \"Test\" and put test pictures in it, following this also make a \"result\" directory\n",
    "\n",
    "After these steps you can teach the model.\n",
    "\n",
    "## Problem: Colorizing black and white pictures (with user interactions)\n",
    "\n",
    "As we mentioned in our previous milestone we would like to create a neural network which can colorize black and white pictures. In adition to this we would like to provide a possibility for the user to change the result (colorized) image by giving the color of certain points in the original (black and white) image.\n",
    "\n",
    "At the moment we have a dataset of ~70k images. (we wrote a script in the previous milestone, this time we will only upload the pictures in a .zip).\n",
    "\n",
    "### The change in the data\n",
    "We have used anther image dataset, since we found a relatively good network in an article that uses 256x256 sized pictures for colorization. This dataset contains approximately 9k images, we imagined that this set will be suitable for our first model.\n",
    "_Note:_ Also we changed the names of the pictures to numbers like 1,2,3... to make it easier to load and use them, we will describe this decision in detail later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required imports for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.engine import Layer\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import RepeatVector, Permute\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network\n",
    "\n",
    "We found the basic network in an article. We decided to start from this as they got a relatively nice result with it.\n",
    "It builds up from three main parts:\n",
    "* The __encoder__\n",
    "* The __fusion layer__\n",
    "* The __decoder__\n",
    "\n",
    "### Encoder\n",
    "The decoder is made up from 2D Convolutional layers. It gets the 256x256 grayscale image. This part does the feature extraction, it starts from extracting basic shapes (e.g. lines, curves) and eventually gets a bit more complex patterns in the end. At this time we have got lots of 32x32 images (256 exactly), all containing some pattern.\n",
    "\n",
    "### Fusion layer\n",
    "The article at this point uses a trick to improve the generalization ability of the network. The idea is to push these 32x32 images (with the patterns) through the inception resnet v2 network which is one of the most powerful classifiers nowadays (it can classify the pictures into 1000 classes).\n",
    "\n",
    "With this method we can teach the network some basic pattern colors (like leaves, trees are green, clouds are white etc...) \n",
    "\n",
    "The article calls this the fusion layer, hence it combines this with the original picture data and gives as a second input. \n",
    "\n",
    "### Decoder\n",
    "The last layer is supposed to build up images with the original size (256x256) from the 32x32 images. At the end of the decoder we have 256x256 images with two channels (ab from the Lab image coding), this is the final outputs of the network.\n",
    "\n",
    "### Model with funcition API\n",
    "We have built our model with the Keras functional API, becasuse of the complexity of the model. This API makes easier to create models for example with multi-input and multi-output or shared layers, and in this project, we have to merge the resnet v2 model's prediction and the convolutional model's prediction, so we have to deal with multi-in/outputs. We use this API to create the model with the three part, the encoder, the fusion and the decoder part. This API seems to be also usefull in the future, if we would like to use our model on videos instead of pictures.\n",
    "\n",
    "![alt text](structure.png \"Structure\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion input\n",
    "embed_input = Input(shape=(1000,), name='embed_input')\n",
    "\n",
    "# Encoder\n",
    "encoder_input = Input(shape=(256, 256, 1,), name='encoder_input')\n",
    "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "\n",
    "# Fusion\n",
    "fusion_output = RepeatVector(32 * 32)(embed_input) \n",
    "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
    "fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
    "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output) \n",
    "\n",
    "# Decoder\n",
    "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the images through _inception resnet v2_\n",
    "\n",
    "This is the function that resizes the given image to 299x299 since the inception resnet v2 uses this size.\n",
    "As a result the function returns prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inception_embedding(grayscaled_rgb):\n",
    "    grayscaled_rgb_resized = []\n",
    "    for i in grayscaled_rgb:\n",
    "        i = resize(i, (299, 299, 3), mode='constant')\n",
    "        #print(i.shape)\n",
    "        grayscaled_rgb_resized.append(i)\n",
    "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
    "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
    "    with inception.graph.as_default():\n",
    "        embed = inception.predict(grayscaled_rgb_resized)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load weights\n",
    "inception = InceptionResNetV2(weights='imagenet', include_top=True)\n",
    "inception.graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running out of memory while loading the pictures\n",
    "\n",
    "So we realized that we could not load in all the 70k pictures at the same time because they would consume too much memory. However the pictures use only 750 MB space, if we load them into 256x256x3 sized tensors (for the 256x256 pixels and the 3 color channels) they need 256 * 256 * 3 * 4 bytes = 786 432 bytes = ~0.8 MB. (since we use float numbers). SO if we would like to load in 70k pictures and each one of them uses 0.8 MB space we would need ~56 000 MB = 56 GB memory. It is almost impossible to have that much memory (especially if we use GPU for training), so we load in only batch-size pictures at a single time. We wrote a new class to handle the picture loading: __DataGenerator__\n",
    "\n",
    "### Training with generator:\n",
    "\n",
    "We found out that there is a way in keras to train the network with batches of data. The _Sequential.fit_generator()_ method trains the network with data which is generated __batch-by-batch__ by a Python generator. We implemented a Python generator, the class called __DataGenerator__.\n",
    "\n",
    "Before we can explain the class, we shall discuss another part of the code, which we created for the same manner as the generator class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using indexes to access the data\n",
    "The main problem is the earlier mentioned size of the dataset. If we want to load the data for splitting it into train, validation and test sets, we immediately bump into the not enough memory problem. Instead of this, we should assing __indexes__ to the data. Then if we split the indexes, the separation of the data is solved. We can make partitions from the indexes (_train, validation, test_).\n",
    "\n",
    "The least complicated way to assign ids to the files, is giving names to the files same as the ids. Accordingly we did this in advance. We can easily do this with shell scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The range of existing file names (therefore ids)\n",
    "all_id = list(range(1000, 10293))\n",
    "\n",
    "#The split proportions\n",
    "valid_split = 0.1\n",
    "test_split = 0.1\n",
    "\n",
    "#The split indexes\n",
    "v_index = int(len(all_id)*(1-valid_split-test_split))\n",
    "t_index = int(len(all_id)*(1-test_split))\n",
    "\n",
    "#Splitting the id sets\n",
    "train_ids = all_id[:v_index]\n",
    "valid_ids = all_id[v_index:t_index]\n",
    "test_ids = all_id[t_index:]\n",
    "\n",
    "#Printing the length of the id sets\n",
    "print(\"Length of train set: \" + str(len(train_ids)))\n",
    "print(\"Length of validation set: \" + str(len(valid_ids)))\n",
    "print(\"Length of test set: \" + str(len(test_ids)))\n",
    "\n",
    "#Storing them for later usage\n",
    "partition = {'train': train_ids, 'validation': valid_ids, 'test': test_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataGenerator class\n",
    "This class does the input data generation in batches.\n",
    "\n",
    "The class stores the crucial parameters of the generation (e.g. batch size, list of ids -therefore filenames-, shuffle option). The most important methods are __getitem__ and __data_generation__ since these are responsible for the generation of the data.\n",
    "\n",
    "Comments describe the behaviour of the class in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    # Initialization\n",
    "    def __init__(self, list_IDs, batch_size=32, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    # Denotes the number of batches per epoch\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    # Generate one batch of data\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    # Updates indexes after each epoch\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    # Generates data containing batch_size samples\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # Initialization (X for imput gray images, X for output ab channel images, I for resnet prediction) \n",
    "        X = np.empty((self.batch_size, *(256,256,1)), dtype=float)\n",
    "        Y = np.empty((self.batch_size, *(256,256,2)), dtype=float)\n",
    "        I = np.empty((self.batch_size, 1000))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Loading image\n",
    "            img = img_to_array(load_img('floydhub_dataset/' + str(ID) + '.jpg'))\n",
    "            # Convert to grayscale\n",
    "            grayscaled_rgb = gray2rgb(rgb2gray(img))\n",
    "            grayscaled_rgb = grayscaled_rgb.reshape((1,*(grayscaled_rgb.shape)))\n",
    "            # Setting the right domain for Lab converting \n",
    "            img = 1.0/255*img\n",
    "            # Convert image to Lab\n",
    "            img = rgb2lab(img)\n",
    "            # Separating the image\n",
    "            gray = img[:,:,0]\n",
    "            ab = img[:,:,1:] / 128\n",
    "            \n",
    "            # Inception resnet prediction input\n",
    "            I[i,] = create_inception_embedding(grayscaled_rgb)\n",
    "            \n",
    "            # Image input\n",
    "            X[i,] = gray.reshape((256,256,1))\n",
    "\n",
    "            # Output\n",
    "            Y[i,] = ab\n",
    "\n",
    "        # Returning with the multiple input and the output\n",
    "        return {'encoder_input': X, 'embed_input': I}, Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the network\n",
    "After all these preparations we can finally start the training of the network (with the generator).\n",
    "\n",
    "We used early stopping in the training of the model.\n",
    "\n",
    "### About the batch_size and steps_per_epoch\n",
    "We have already given _train and validation_ sets. The __batch_size__ could be chosen willingly, however after this choice, the __steps_per_epoch__ should be approximately _train set size/batch_size_ so for example if we have a training set containing 1000 images and a batch_size of 100, afterwards the steps_per_epoch shall be 1000/100 = 10.\n",
    "\n",
    "__validation_steps__ parameter is the same for the _valid_generator_ and the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "patience=10\n",
    "early_stopping=EarlyStopping(patience=patience, verbose=1)\n",
    "checkpointer=ModelCheckpoint(filepath='weights.hdf5', save_best_only=True, verbose=1)\n",
    "\n",
    "\n",
    "# The generator which generates the training data\n",
    "training_generator = DataGenerator(partition['train'], batch_size=1)\n",
    "# The generator which generates the validation data\n",
    "valid_generator = DataGenerator(partition['validation'], batch_size=100)\n",
    "\n",
    "\n",
    "# Train model      \n",
    "model.compile(optimizer='rmsprop', loss='mse')\n",
    "model.fit_generator(training_generator, validation_data=valid_generator,\n",
    "                    epochs=100, steps_per_epoch=148, validation_steps=18,\n",
    "                   callbacks=[checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the network and evaluating the results\n",
    "\n",
    "### Testing\n",
    "For the Milestone 2, we load the test images from a directory, called \"Test\", instead of the created test_split, because we would like to test on different, but certain pictures, to test our model in different cases.\n",
    "\n",
    "### Evaluation\n",
    "To evaluate the network at this time of the project we only use the keras __Model.evaluate()__, which describes the loss and other metrics for the model. In addition we of course use our eyes for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test images\n",
    "tests = []\n",
    "for filename in os.listdir('Test/'):\n",
    "    tests.append(img_to_array(load_img('Test/'+filename)))\n",
    "tests = np.array(color_me, dtype=float)\n",
    "gray_me = gray2rgb(rgb2gray(1.0/255*tests))\n",
    "color_me_embed = create_inception_embedding(gray_me)\n",
    "X_test = rgb2lab(1.0/255*tests)[:,:,:,0]\n",
    "X_test = X_test.reshape(X_test.shape+(1,))\n",
    "Y_test = rgb2lab(1.0/255*tests)[:,:,:,1:]\n",
    "Y_test = Y_test / 128\n",
    "\n",
    "print(model.evaluate([X_test, color_me_embed], Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_me = []\n",
    "for filename in os.listdir('Test/'):\n",
    "    color_me.append(img_to_array(load_img('Test/'+filename)))\n",
    "color_me = np.array(color_me, dtype=float)\n",
    "gray_me = gray2rgb(rgb2gray(1.0/255*color_me))\n",
    "color_me_embed = create_inception_embedding(gray_me)\n",
    "color_me = rgb2lab(1.0/255*color_me)[:,:,:,0]\n",
    "color_me = color_me.reshape(color_me.shape+(1,))\n",
    "\n",
    "\n",
    "# Test model\n",
    "output = model.predict([color_me, color_me_embed])\n",
    "output = output * 128\n",
    "\n",
    "# Output colorizations\n",
    "for i in range(len(output)):\n",
    "    cur = np.zeros((256, 256, 3))\n",
    "    cur[:,:,0] = color_me[i][:,:,0]\n",
    "    cur[:,:,1:] = output[i]\n",
    "    imsave(\"result/img_\"+str(i)+\".png\", lab2rgb(cur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and teaching the network\n",
    "We trained the network on circa 7400 images and validate it on roughly 900 images, with 20 epochs. You can see the process on this picture:\n",
    "\n",
    "![alt text](training.png \"Training\")\n",
    "\n",
    "It took harshly 3 and a half hours on a Nvidia GTX1070 GPU.\n",
    "\n",
    "### The test results can be seen in the \"colorizing_results\" folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "With this first model we can reach some not too outstanding results but they are definitely worth mentioning, the model started to color the pictures although it probably needs some more data and time to learn, moreover we can probably satisfy these needs in the future.\n",
    "\n",
    "The model is quite large, copied to the GPU, it is estimately 7.2GB, so decreasing the size is a reasonable goal, with an eye to the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User interactions\n",
    "\n",
    "In this version of the program the user interaction part has not been implemented yet. First we would like to see a good result in the simple colorization part. \n",
    "\n",
    "In the final version we will give the generated user inputs (see Milestone I.) to the network during training so we hope that this way it can learn how to use the given user inputs later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
