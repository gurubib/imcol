{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The imports\n",
    "\n",
    "Basically we use the keras library for the problem. In addition to this we use scikit-image for image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.engine import Layer\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import RepeatVector, Permute\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic variables\n",
    "\n",
    "In this section we set the basic variables for the data, like dataset size, valid ant test split.\n",
    "(We decided to create a variable for the data size because we didn't run the training on the whole dataset each time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 56000\n",
      "Length of validation set: 7000\n",
      "Length of test set: 7000\n"
     ]
    }
   ],
   "source": [
    "dataset_size = 70000\n",
    "\n",
    "#Since we renamed the images to numbers from 1 to ~70k we use a range to index the images.\n",
    "ids = list(range(0, 70000))\n",
    "#Shuffling the ids into a random order to shuffle the images.\n",
    "random.shuffle(ids)\n",
    "\n",
    "all_id = ids[:dataset_size]\n",
    "\n",
    "#Test/Valid splits\n",
    "valid_split = 0.1\n",
    "test_split = 0.1\n",
    "\n",
    "v_index = int(len(all_id)*(1-valid_split-test_split))\n",
    "t_index = int(len(all_id)*(1-test_split))\n",
    "\n",
    "#Setting the splits\n",
    "train_ids = all_id[:v_index]\n",
    "valid_ids = all_id[v_index:t_index]\n",
    "test_ids = all_id[t_index:]\n",
    "\n",
    "print(\"Length of train set: \" + str(len(train_ids)))\n",
    "print(\"Length of validation set: \" + str(len(valid_ids)))\n",
    "print(\"Length of test set: \" + str(len(test_ids)))\n",
    "\n",
    "partition = {'train': train_ids, 'validation': valid_ids, 'test': test_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DataGenerator class\n",
    "\n",
    "You can read the details of it in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=10, dim=(256,256,1), shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *(256,256,1)), dtype=float)\n",
    "        Y = np.empty((self.batch_size, *(256,256,2)), dtype=float)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            img = img_to_array(load_img('C:/Users/lakas/Documents/deeplearning/imcol-master/dataset/' + str(ID) + '.jpg'))\n",
    "            img = 1.0/255*img\n",
    "            img = rgb2lab(img)\n",
    "            \n",
    "            gray = img[:,:,0] \n",
    "            ab = img[:,:,1:] / 128\n",
    "            \n",
    "            # Store sample\n",
    "            X[i,] = gray.reshape((256,256,1))\n",
    "\n",
    "            # Store class\n",
    "            Y[i,] = ab\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "we used an early stopping with a patienc of 15. In addition to preventing the network from overfitting it also made our program more robust since in case of an error during the training we didn't loose the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience=15\n",
    "early_stopping=EarlyStopping(patience=patience, verbose=1)\n",
    "checkpointer=ModelCheckpoint(filepath='weights.hdf5', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The swish activation\n",
    "\n",
    "Here you can see the implementation of the swish function. More details about the function in the documentary.\n",
    "The Swish class is a specialization of the Activation class. You can reach it in the keras methods just like you can e.g 'relu'. (it's name is: 'swish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "class Swish(Activation):\n",
    "    \n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Swish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'swish'\n",
    "\n",
    "def swish(x):\n",
    "    A = (K.sigmoid(x) * x)\n",
    "    return A\n",
    "\n",
    "get_custom_objects().update({'swish': Swish(swish)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here we build the network with the given activation in all layers.\n",
    "In our opinion this is the best resulting network, but since it is subjective which result images you think are the best you can find the differnet networks' results in the documentary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = 'swish'\n",
    "\n",
    "#Encoder\n",
    "encoder_input = Input(shape=(256, 256, 1,), name='encoder_input')\n",
    "encoder_output = Conv2D(64, (3,3), activation=act, padding='same', strides=2)(encoder_input)\n",
    "encoder_output = Conv2D(128, (3,3), activation=act, padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(128, (3,3), activation=act, padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation=act, padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation=act, padding='same', strides=2)(encoder_output)\n",
    "\n",
    "#Bottle-neck\n",
    "encoder_output = Conv2D(512, (3,3), activation=act, padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation=act, padding='same')(encoder_output)\n",
    "\n",
    "\n",
    "#Decoder\n",
    "encoder_output = Conv2D(256, (3,3), activation=act, padding='same')(encoder_output)\n",
    "decoder_output = Conv2D(128, (3,3), activation=act, padding='same')(encoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(64, (3,3), activation=act, padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(32, (3,3), activation=act, padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(16, (3,3), activation=act, padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "\n",
    "model = Model(inputs=encoder_input, outputs=decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the generators and starting the training\n",
    "\n",
    "We used a batch-size of 50, adam optimizer, and mean_absolute_error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1120/1120 [==============================] - 1305s 1s/step - loss: 0.1024 - val_loss: 0.0960\n",
      "Epoch 2/40\n",
      "1120/1120 [==============================] - 1261s 1s/step - loss: 0.0960 - val_loss: 0.0955\n",
      "Epoch 3/40\n",
      "1120/1120 [==============================] - 1252s 1s/step - loss: 0.0957 - val_loss: 0.0959\n",
      "Epoch 4/40\n",
      "1120/1120 [==============================] - 1248s 1s/step - loss: 0.0949 - val_loss: 0.0942\n",
      "Epoch 5/40\n",
      "1120/1120 [==============================] - 1247s 1s/step - loss: 0.0943 - val_loss: 0.0940\n",
      "Epoch 6/40\n",
      "1120/1120 [==============================] - 1249s 1s/step - loss: 0.0937 - val_loss: 0.0934\n",
      "Epoch 7/40\n",
      "1120/1120 [==============================] - 1248s 1s/step - loss: 0.0934 - val_loss: 0.0926\n",
      "Epoch 8/40\n",
      "1120/1120 [==============================] - 1254s 1s/step - loss: 0.0929 - val_loss: 0.0922\n",
      "Epoch 9/40\n",
      "1120/1120 [==============================] - 1246s 1s/step - loss: 0.0924 - val_loss: 0.0916\n",
      "Epoch 10/40\n",
      "1120/1120 [==============================] - 1245s 1s/step - loss: 0.0925 - val_loss: 0.0930\n",
      "Epoch 11/40\n",
      "1120/1120 [==============================] - 1251s 1s/step - loss: 0.0912 - val_loss: 0.0911\n",
      "Epoch 12/40\n",
      "1120/1120 [==============================] - 1246s 1s/step - loss: 0.0898 - val_loss: 0.0896\n",
      "Epoch 13/40\n",
      "1120/1120 [==============================] - 1255s 1s/step - loss: 0.0891 - val_loss: 0.0888\n",
      "Epoch 14/40\n",
      "1120/1120 [==============================] - 1250s 1s/step - loss: 0.0882 - val_loss: 0.0881\n",
      "Epoch 15/40\n",
      "1120/1120 [==============================] - 1246s 1s/step - loss: 0.0874 - val_loss: 0.0889\n",
      "Epoch 16/40\n",
      "1120/1120 [==============================] - 1247s 1s/step - loss: 0.0867 - val_loss: 0.0878\n",
      "Epoch 17/40\n",
      "1120/1120 [==============================] - 1247s 1s/step - loss: 0.0860 - val_loss: 0.0869\n",
      "Epoch 18/40\n",
      "1120/1120 [==============================] - 1249s 1s/step - loss: 0.0851 - val_loss: 0.0868\n",
      "Epoch 19/40\n",
      "1120/1120 [==============================] - 1245s 1s/step - loss: 0.0843 - val_loss: 0.0867\n",
      "Epoch 20/40\n",
      "1120/1120 [==============================] - 1250s 1s/step - loss: 0.0831 - val_loss: 0.0868\n",
      "Epoch 21/40\n",
      "1120/1120 [==============================] - 1245s 1s/step - loss: 0.0819 - val_loss: 0.0875\n",
      "Epoch 22/40\n",
      "1120/1120 [==============================] - 1245s 1s/step - loss: 0.0806 - val_loss: 0.0865\n",
      "Epoch 23/40\n",
      "1120/1120 [==============================] - 1245s 1s/step - loss: 0.0790 - val_loss: 0.0872\n",
      "Epoch 24/40\n",
      "1120/1120 [==============================] - 1262s 1s/step - loss: 0.0777 - val_loss: 0.0875\n",
      "Epoch 25/40\n",
      "1120/1120 [==============================] - 1276s 1s/step - loss: 0.0761 - val_loss: 0.0878\n",
      "Epoch 26/40\n",
      "1120/1120 [==============================] - 1265s 1s/step - loss: 0.0745 - val_loss: 0.0884\n",
      "Epoch 27/40\n",
      "1120/1120 [==============================] - 1266s 1s/step - loss: 0.0730 - val_loss: 0.0891\n",
      "Epoch 28/40\n",
      "1120/1120 [==============================] - 1271s 1s/step - loss: 0.0715 - val_loss: 0.0884\n",
      "Epoch 29/40\n",
      "1120/1120 [==============================] - 1270s 1s/step - loss: 0.0701 - val_loss: 0.0891\n",
      "Epoch 30/40\n",
      "1120/1120 [==============================] - 1265s 1s/step - loss: 0.0689 - val_loss: 0.0885\n",
      "Epoch 31/40\n",
      "1120/1120 [==============================] - 1269s 1s/step - loss: 0.0677 - val_loss: 0.0898\n",
      "Epoch 32/40\n",
      "1120/1120 [==============================] - 1266s 1s/step - loss: 0.0666 - val_loss: 0.0894\n",
      "Epoch 33/40\n",
      "1120/1120 [==============================] - 1272s 1s/step - loss: 0.0656 - val_loss: 0.0896\n",
      "Epoch 34/40\n",
      "1120/1120 [==============================] - 1334s 1s/step - loss: 0.0647 - val_loss: 0.0905\n",
      "Epoch 35/40\n",
      "1120/1120 [==============================] - 1336s 1s/step - loss: 0.0652 - val_loss: 0.0907\n",
      "Epoch 36/40\n",
      "1120/1120 [==============================] - 1336s 1s/step - loss: 0.0633 - val_loss: 0.0908\n",
      "Epoch 37/40\n",
      "1120/1120 [==============================] - 1334s 1s/step - loss: 0.0623 - val_loss: 0.0908\n",
      "Epoch 38/40\n",
      "1120/1120 [==============================] - 1318s 1s/step - loss: 0.0616 - val_loss: 0.0903\n",
      "Epoch 39/40\n",
      "1120/1120 [==============================] - 1321s 1s/step - loss: 0.0611 - val_loss: 0.0908\n",
      "Epoch 40/40\n",
      "1120/1120 [==============================] - 1318s 1s/step - loss: 0.0603 - val_loss: 0.0921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17a0b300b00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_generator = DataGenerator(partition['train'], batch_size=50)\n",
    "valid_generator = DataGenerator(partition['validation'], batch_size=50)\n",
    "# Train model      \n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "model.fit_generator(training_generator, validation_data=valid_generator,\n",
    "                    epochs=40, steps_per_epoch=len(train_ids)//50, validation_steps=len(valid_ids)//50)#,\n",
    "                   #callbacks=[checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "\n",
    "We both save the model into a .json file and the weights into an .hdf5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"weights.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "\n",
    "We load the model from the json file in order to be able to run the testing without running the whole program again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "import json\n",
    "\n",
    "with open('model.json') as json_data:\n",
    "    d = json_data.read()\n",
    "\n",
    "\n",
    "json_string = d\n",
    "model = model_from_json(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "\n",
    "It gets the test images from the Test/ directory and place the results into the result/ directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to '/data/images/Test/' to use all the 500 images\n",
    "\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "color_me = []\n",
    "for filename in os.listdir('Test/'):\n",
    "\tcolor_me.append(img_to_array(load_img('Test/'+filename)))\n",
    "color_me = np.array(color_me, dtype=float)\n",
    "color_me = rgb2lab(1.0/255*color_me)[:,:,:,0] #Only using the gray channel as the input\n",
    "color_me = color_me.reshape(color_me.shape+(1,))\n",
    "\n",
    "# Test model\n",
    "#output = model.predict(color_me / 100) #Norm\n",
    "output = model.predict(color_me) #No Norm\n",
    "output = output * 128\n",
    "\n",
    "# Output colorizations\n",
    "for i in range(len(output)):\n",
    "\tcur = np.zeros((256, 256, 3))\n",
    "\tcur[:,:,0] = color_me[i][:,:,0]\n",
    "\tcur[:,:,1:] = output[i]\n",
    "\timsave(\"result/img_\"+str(i)+\".png\", lab2rgb(cur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 256, 256, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 64)      640       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 128)       295040    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 64, 64)        73792     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 128, 128, 32)      18464     \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 128, 128, 16)      4624      \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 128, 128, 2)       290       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 256, 256, 2)       0         \n",
      "=================================================================\n",
      "Total params: 6,219,410\n",
      "Trainable params: 6,219,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model memory usage\n",
    "\n",
    "We found a little code that prints out the memory usage of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.772"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_memory_usage(batch_size, model):\n",
    "    from keras import backend as K\n",
    "\n",
    "    shapes_mem_count = 0\n",
    "    for l in model.layers:\n",
    "        single_layer_mem = 1\n",
    "        for s in l.output_shape:\n",
    "            if s is None:\n",
    "                continue\n",
    "            single_layer_mem *= s\n",
    "        shapes_mem_count += single_layer_mem\n",
    "\n",
    "    trainable_count = np.sum([K.count_params(p) for p in set(model.trainable_weights)])\n",
    "    non_trainable_count = np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])\n",
    "\n",
    "    total_memory = 4.0*batch_size*(shapes_mem_count + trainable_count + non_trainable_count)\n",
    "    gbytes = np.round(total_memory / (1024.0 ** 3), 3)\n",
    "    return gbytes\n",
    "\n",
    "get_model_memory_usage(100, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
